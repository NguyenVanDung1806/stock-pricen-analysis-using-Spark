{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark import SparkContext , SparkConf\n",
    "from pyspark.sql import SparkSession"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = SparkSession\\\n",
    "    .builder\\\n",
    "    .appName(\"Project_Stocks\")\\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "#reading csv data \n",
    "stocks = spark.read.csv(\"StockData\" , header = \"True\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+----------+----------+-------+--------+--------+--------+\n",
      "|Ticker|      Date|Close/Last| Volume|    Open|    High|     Low|\n",
      "+------+----------+----------+-------+--------+--------+--------+\n",
      "| BRK-B|05/31/2023|  $321.08 |6175417|$321.12 |$322.41 |$319.39 |\n",
      "| BRK-B|05/30/2023|  $322.19 |3232461|$321.86 |$322.47 |$319.00 |\n",
      "| BRK-B|05/26/2023|  $320.60 |3229873|$320.44 |$322.63 |$319.67 |\n",
      "| BRK-B|05/25/2023|  $319.02 |4251935|$320.56 |$320.56 |$317.71 |\n",
      "| BRK-B|05/24/2023|  $320.20 |3075393|$322.71 |$323.00 |$319.56 |\n",
      "+------+----------+----------+-------+--------+--------+--------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "stocks.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+----------+--------+\n",
      "|Ticker|      Date|    Open|\n",
      "+------+----------+--------+\n",
      "| BRK-B|05/31/2023|$321.12 |\n",
      "| BRK-B|05/30/2023|$321.86 |\n",
      "| BRK-B|05/26/2023|$320.44 |\n",
      "| BRK-B|05/25/2023|$320.56 |\n",
      "| BRK-B|05/24/2023|$322.71 |\n",
      "+------+----------+--------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "stocks.select([\"Ticker\", \"Date\" , \"Open\"]).show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+----------+----------+--------+--------+--------+--------+\n",
      "|Ticker|      Date|Close/Last|  Volume|    Open|    High|     Low|\n",
      "+------+----------+----------+--------+--------+--------+--------+\n",
      "|  MSFT|05/31/2023|  $328.39 |45950550|$332.29 |$335.94 |$327.33 |\n",
      "|  MSFT|05/30/2023|  $331.21 |29503070|$335.23 |$335.74 |$330.52 |\n",
      "|  MSFT|05/26/2023|  $332.89 |36630630|$324.02 |$333.40 |$323.88 |\n",
      "|  MSFT|05/25/2023|  $325.92 |43301740|$323.24 |$326.90 |$320.00 |\n",
      "|  MSFT|05/24/2023|  $313.85 |23384890|$314.73 |$316.50 |$312.61 |\n",
      "|  MSFT|05/23/2023|  $315.26 |30797170|$320.03 |$322.72 |$315.25 |\n",
      "|  MSFT|05/22/2023|  $321.18 |24115660|$318.60 |$322.59 |$318.01 |\n",
      "|  MSFT|05/19/2023|  $318.34 |27546700|$316.74 |$318.75 |$316.37 |\n",
      "|  MSFT|05/18/2023|  $318.52 |27275990|$314.53 |$319.04 |$313.72 |\n",
      "|  MSFT|05/17/2023|  $314.00 |24315010|$312.29 |$314.43 |$310.74 |\n",
      "|  MSFT|05/16/2023|  $311.74 |26730350|$309.83 |$313.71 |$309.83 |\n",
      "|  MSFT|05/15/2023|  $309.46 |16336550|$309.10 |$309.91 |$307.59 |\n",
      "|  MSFT|05/12/2023|  $308.97 |19774700|$310.55 |$310.65 |$306.60 |\n",
      "|  MSFT|05/11/2023|  $310.11 |31680180|$310.10 |$311.12 |$306.26 |\n",
      "|  MSFT|05/10/2023|  $312.31 |30078040|$308.62 |$313.00 |$307.67 |\n",
      "|  MSFT|05/09/2023|  $307.00 |21340830|$308.00 |$310.04 |$306.31 |\n",
      "|  MSFT|05/08/2023|  $308.65 |21318610|$310.13 |$310.20 |$306.09 |\n",
      "|  MSFT|05/05/2023|  $310.65 |28197050|$305.72 |$311.97 |$304.27 |\n",
      "|  MSFT|05/04/2023|  $305.41 |22519910|$306.24 |$307.76 |$303.40 |\n",
      "|  MSFT|05/03/2023|  $304.40 |22360750|$306.62 |$308.61 |$304.09 |\n",
      "+------+----------+----------+--------+--------+--------+--------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "stocks.filter(stocks.Ticker == \"MSFT\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+----------+----------+--------+--------+--------+--------+\n",
      "|Ticker|      Date|Close/Last|  Volume|    Open|    High|     Low|\n",
      "+------+----------+----------+--------+--------+--------+--------+\n",
      "|  MSFT|05/26/2023|  $332.89 |36630630|$324.02 |$333.40 |$323.88 |\n",
      "+------+----------+----------+--------+--------+--------+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "stocks.filter((stocks.Ticker == \"MSFT\") & (stocks.Date == \"05/26/2023\")).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+----------+----------+--------+--------+--------+--------+\n",
      "|Ticker|      Date|Close/Last|  Volume|    Open|    High|     Low|\n",
      "+------+----------+----------+--------+--------+--------+--------+\n",
      "|  MSFT|05/31/2023|  $328.39 |45950550|$332.29 |$335.94 |$327.33 |\n",
      "|     V|05/31/2023|  $221.03 |20460620|$219.96 |$221.53 |$216.14 |\n",
      "+------+----------+----------+--------+--------+--------+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "stocks.filter(((stocks.Ticker == \"MSFT\") | (stocks.Ticker == \"V\") ) & (stocks.Date == \"05/31/2023\")).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+----------+----------+---------+--------+--------+--------+\n",
      "|Ticker|      Date|Close/Last|   Volume|    Open|    High|     Low|\n",
      "+------+----------+----------+---------+--------+--------+--------+\n",
      "|  MSFT|05/31/2023|  $328.39 | 45950550|$332.29 |$335.94 |$327.33 |\n",
      "|  TSLA|05/31/2023|  $203.93 |150711700|$199.78 |$203.95 |$195.12 |\n",
      "|     V|05/31/2023|  $221.03 | 20460620|$219.96 |$221.53 |$216.14 |\n",
      "|   SPY|05/31/2023|    417.85|110811800|  418.28|  419.22|  416.22|\n",
      "|   QQQ|05/31/2023|    347.99| 65105380|  348.37|   350.6|  346.51|\n",
      "+------+----------+----------+---------+--------+--------+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "stocks.filter((stocks.Ticker.isin(\"MSFT\" , \"QQQ\" , \"SPY\" , \"V\" ,   \"TSLA\")) & (stocks.Date == \"05/31/2023\")).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- Ticker: string (nullable = true)\n",
      " |-- Date: string (nullable = true)\n",
      " |-- Close/Last: string (nullable = true)\n",
      " |-- Volume: string (nullable = true)\n",
      " |-- Open: string (nullable = true)\n",
      " |-- High: string (nullable = true)\n",
      " |-- Low: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "stocks.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import udf\n",
    "from pyspark.sql.types import DateType\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "def date_parser(date_str):\n",
    "    try:\n",
    "        return datetime.strptime(date_str, \"%m/%d/%Y\")\n",
    "    except ValueError:\n",
    "        return None  # Xử lý các giá trị không hợp lệ\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "date_parser_udf = udf(date_parser, DateType())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+----------+----------+-------+--------+--------+--------+----------+\n",
      "|Ticker|      Date|Close/Last| Volume|    Open|    High|     Low|ParsedDate|\n",
      "+------+----------+----------+-------+--------+--------+--------+----------+\n",
      "| BRK-B|05/31/2023|  $321.08 |6175417|$321.12 |$322.41 |$319.39 |2023-05-31|\n",
      "| BRK-B|05/30/2023|  $322.19 |3232461|$321.86 |$322.47 |$319.00 |2023-05-30|\n",
      "| BRK-B|05/26/2023|  $320.60 |3229873|$320.44 |$322.63 |$319.67 |2023-05-26|\n",
      "| BRK-B|05/25/2023|  $319.02 |4251935|$320.56 |$320.56 |$317.71 |2023-05-25|\n",
      "| BRK-B|05/24/2023|  $320.20 |3075393|$322.71 |$323.00 |$319.56 |2023-05-24|\n",
      "| BRK-B|05/23/2023|  $323.11 |4031342|$328.19 |$329.27 |$322.97 |2023-05-23|\n",
      "| BRK-B|05/22/2023|  $329.13 |2763422|$330.75 |$331.49 |$328.35 |2023-05-22|\n",
      "| BRK-B|05/19/2023|  $330.39 |4323538|$331.00 |$333.94 |$329.12 |2023-05-19|\n",
      "| BRK-B|05/18/2023|  $329.76 |2808329|$326.87 |$329.98 |$325.85 |2023-05-18|\n",
      "| BRK-B|05/17/2023|  $327.39 |3047626|$325.02 |$328.26 |$324.82 |2023-05-17|\n",
      "| BRK-B|05/16/2023|  $323.75 |2139996|$322.46 |$324.69 |$322.36 |2023-05-16|\n",
      "| BRK-B|05/15/2023|  $323.53 |2191609|$322.89 |$323.83 |$320.13 |2023-05-15|\n",
      "| BRK-B|05/12/2023|  $322.49 |1938264|$323.82 |$324.24 |$320.54 |2023-05-12|\n",
      "| BRK-B|05/11/2023|  $322.64 |2549339|$321.00 |$322.96 |$319.81 |2023-05-11|\n",
      "| BRK-B|05/10/2023|  $322.99 |2641134|$326.08 |$326.16 |$320.15 |2023-05-10|\n",
      "| BRK-B|05/09/2023|  $324.87 |2285924|$324.87 |$326.88 |$323.48 |2023-05-09|\n",
      "| BRK-B|05/08/2023|  $326.14 |3303393|$328.26 |$330.69 |$325.79 |2023-05-08|\n",
      "| BRK-B|05/05/2023|  $323.88 |3876299|$323.36 |$325.16 |$322.62 |2023-05-05|\n",
      "| BRK-B|05/04/2023|  $320.00 |3194768|$323.44 |$325.99 |$317.41 |2023-05-04|\n",
      "| BRK-B|05/03/2023|  $323.22 |2660456|$327.13 |$328.07 |$323.06 |2023-05-03|\n",
      "+------+----------+----------+-------+--------+--------+--------+----------+\n",
      "only showing top 20 rows\n",
      "\n",
      "root\n",
      " |-- Ticker: string (nullable = true)\n",
      " |-- Date: string (nullable = true)\n",
      " |-- Close/Last: string (nullable = true)\n",
      " |-- Volume: string (nullable = true)\n",
      " |-- Open: string (nullable = true)\n",
      " |-- High: string (nullable = true)\n",
      " |-- Low: string (nullable = true)\n",
      " |-- ParsedDate: date (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    stocks = stocks.withColumn(\"ParsedDate\", date_parser_udf(stocks[\"Date\"]))\n",
    "    stocks.show()\n",
    "    stocks.printSchema()\n",
    "except Exception as e:\n",
    "    print(\"Error:\", e)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+----------+----------+-------+------+------+------+----------+\n",
      "|Ticker|      Date|Close/Last| Volume|  Open|  High|   Low|ParsedDate|\n",
      "+------+----------+----------+-------+------+------+------+----------+\n",
      "| BRK-B|05/31/2023|    321.08|6175417|321.12|322.41|319.39|2023-05-31|\n",
      "| BRK-B|05/30/2023|    322.19|3232461|321.86|322.47| 319.0|2023-05-30|\n",
      "| BRK-B|05/26/2023|     320.6|3229873|320.44|322.63|319.67|2023-05-26|\n",
      "| BRK-B|05/25/2023|    319.02|4251935|320.56|320.56|317.71|2023-05-25|\n",
      "| BRK-B|05/24/2023|     320.2|3075393|322.71| 323.0|319.56|2023-05-24|\n",
      "| BRK-B|05/23/2023|    323.11|4031342|328.19|329.27|322.97|2023-05-23|\n",
      "| BRK-B|05/22/2023|    329.13|2763422|330.75|331.49|328.35|2023-05-22|\n",
      "| BRK-B|05/19/2023|    330.39|4323538| 331.0|333.94|329.12|2023-05-19|\n",
      "| BRK-B|05/18/2023|    329.76|2808329|326.87|329.98|325.85|2023-05-18|\n",
      "| BRK-B|05/17/2023|    327.39|3047626|325.02|328.26|324.82|2023-05-17|\n",
      "| BRK-B|05/16/2023|    323.75|2139996|322.46|324.69|322.36|2023-05-16|\n",
      "| BRK-B|05/15/2023|    323.53|2191609|322.89|323.83|320.13|2023-05-15|\n",
      "| BRK-B|05/12/2023|    322.49|1938264|323.82|324.24|320.54|2023-05-12|\n",
      "| BRK-B|05/11/2023|    322.64|2549339| 321.0|322.96|319.81|2023-05-11|\n",
      "| BRK-B|05/10/2023|    322.99|2641134|326.08|326.16|320.15|2023-05-10|\n",
      "| BRK-B|05/09/2023|    324.87|2285924|324.87|326.88|323.48|2023-05-09|\n",
      "| BRK-B|05/08/2023|    326.14|3303393|328.26|330.69|325.79|2023-05-08|\n",
      "| BRK-B|05/05/2023|    323.88|3876299|323.36|325.16|322.62|2023-05-05|\n",
      "| BRK-B|05/04/2023|     320.0|3194768|323.44|325.99|317.41|2023-05-04|\n",
      "| BRK-B|05/03/2023|    323.22|2660456|327.13|328.07|323.06|2023-05-03|\n",
      "+------+----------+----------+-------+------+------+------+----------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.types import FloatType\n",
    "from pyspark.sql.functions import col\n",
    "def num_parser(value):\n",
    "    if isinstance(value, str):\n",
    "        return float(value.strip(\"$\"))\n",
    "    elif isinstance(value, (int ,   float)):\n",
    "        return value\n",
    "    else :\n",
    "        return None\n",
    "    \n",
    "parser_number = udf(num_parser , FloatType())\n",
    "\n",
    "stocks = (stocks\n",
    "          .withColumn(\"Open\", parser_number(col(\"Open\")))\n",
    "          .withColumn(\"High\", parser_number(col(\"High\")))\n",
    "          .withColumn(\"Low\", parser_number(col(\"Low\")))\n",
    "          .withColumn(\"Close/Last\", parser_number(col(\"Close/Last\"))))\n",
    "stocks.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- Ticker: string (nullable = true)\n",
      " |-- Date: string (nullable = true)\n",
      " |-- Close/Last: float (nullable = true)\n",
      " |-- Volume: string (nullable = true)\n",
      " |-- Open: float (nullable = true)\n",
      " |-- High: float (nullable = true)\n",
      " |-- Low: float (nullable = true)\n",
      " |-- ParsedDate: date (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "stocks.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "cleaned_stocks = stocks.select([\"Ticker\", \"ParsedDate\", \"Volume\", \"Open\", \"Low\", \"High\", \"Close/Last\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+----------+-------+------+------+------+----------+\n",
      "|Ticker|ParsedDate| Volume|  Open|   Low|  High|Close/Last|\n",
      "+------+----------+-------+------+------+------+----------+\n",
      "| BRK-B|2023-05-31|6175417|321.12|319.39|322.41|    321.08|\n",
      "| BRK-B|2023-05-30|3232461|321.86| 319.0|322.47|    322.19|\n",
      "| BRK-B|2023-05-26|3229873|320.44|319.67|322.63|     320.6|\n",
      "| BRK-B|2023-05-25|4251935|320.56|317.71|320.56|    319.02|\n",
      "| BRK-B|2023-05-24|3075393|322.71|319.56| 323.0|     320.2|\n",
      "+------+----------+-------+------+------+------+----------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "cleaned_stocks.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+--------------------+------------------+------------------+------------------+------------------+\n",
      "|summary|              Volume|              Open|               Low|              High|        Close/Last|\n",
      "+-------+--------------------+------------------+------------------+------------------+------------------+\n",
      "|  count|               15108|             15108|             15108|             15108|             15108|\n",
      "|   mean|5.1868408793685466E7|180.09656566181036| 177.9982781513109| 182.1253348687101| 180.1256089860054|\n",
      "| stddev| 5.496484129953463E7|101.16125813324396|100.26590135955209|101.96625521621728|101.14891782168517|\n",
      "|    min|           100011880|             12.07|              11.8|             12.45|             11.93|\n",
      "|    max|             9999164|            479.22|            476.06|            479.98|            477.71|\n",
      "+-------+--------------------+------------------+------------------+------------------+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "cleaned_stocks.describe([\"Volume\", \"Open\", \"Low\",\"High\",\"Close/Last\"]).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "#bacsic GROUP BY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+---------+\n",
      "|Ticker|max(Open)|\n",
      "+------+---------+\n",
      "| BRK-B|   361.39|\n",
      "|  MSFT|   344.62|\n",
      "|  META|   381.68|\n",
      "|  TSLA|   411.47|\n",
      "|  AAPL|   182.63|\n",
      "|  AMZN|    187.2|\n",
      "| GOOGL|   151.25|\n",
      "|  NVDA|   405.95|\n",
      "|   TSM|   141.61|\n",
      "|     V|   250.05|\n",
      "|   QQQ|   405.57|\n",
      "|   SPY|   479.22|\n",
      "+------+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "cleaned_stocks.groupBy(\"Ticker\").max(\"Open\").show(15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+-------------+\n",
      "|Ticker|MaxStockPrice|\n",
      "+------+-------------+\n",
      "| BRK-B|       361.39|\n",
      "|  MSFT|       344.62|\n",
      "|  META|       381.68|\n",
      "|  TSLA|       411.47|\n",
      "|  AAPL|       182.63|\n",
      "|  AMZN|        187.2|\n",
      "| GOOGL|       151.25|\n",
      "|  NVDA|       405.95|\n",
      "|   TSM|       141.61|\n",
      "|     V|       250.05|\n",
      "|   QQQ|       405.57|\n",
      "|   SPY|       479.22|\n",
      "+------+-------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "cleaned_stocks.groupBy(\"Ticker\").max(\"Open\").withColumnRenamed(\"max(Open)\" , \"MaxStockPrice\").show(15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+---------+\n",
      "|Ticker|max(Open)|\n",
      "+------+---------+\n",
      "| BRK-B|   361.39|\n",
      "|  MSFT|   344.62|\n",
      "|  META|   381.68|\n",
      "|  TSLA|   411.47|\n",
      "|  AAPL|   182.63|\n",
      "|  AMZN|    187.2|\n",
      "| GOOGL|   151.25|\n",
      "|  NVDA|   405.95|\n",
      "|   TSM|   141.61|\n",
      "|     V|   250.05|\n",
      "|   QQQ|   405.57|\n",
      "|   SPY|   479.22|\n",
      "+------+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import udf, col, max as max_ , sum as sum_\n",
    "cleaned_stocks.groupBy(\"Ticker\").agg(max_(\"Open\")).alias(\"MaxStockPrice\").show(15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+-------------+----------------+\n",
      "|Ticker|MaxStockPrice|     TotalVolume|\n",
      "+------+-------------+----------------+\n",
      "| BRK-B|       361.39|   5.862401321E9|\n",
      "|  MSFT|       344.62| 3.7976660472E10|\n",
      "|  META|       381.68| 3.0148848043E10|\n",
      "|  TSLA|       411.47|1.71802975076E11|\n",
      "|  AAPL|       182.63| 1.3931006136E11|\n",
      "|  AMZN|        187.2| 1.0450328743E11|\n",
      "| GOOGL|       151.25| 4.3956560981E10|\n",
      "|  NVDA|       405.95| 5.8787218324E10|\n",
      "|   TSM|       141.61| 1.2506470104E10|\n",
      "|     V|       250.05| 1.0410997871E10|\n",
      "|   QQQ|       405.57| 6.0437153773E10|\n",
      "|   SPY|       479.22|  1.079252853E11|\n",
      "+------+-------------+----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import max as max_ , sum as sum_\n",
    "cleaned_stocks.groupBy(\"Ticker\").agg(\n",
    "    max_(\"Open\").alias(\"MaxStockPrice\"),\n",
    "    sum_(\"Volume\").alias(\"TotalVolume\")\n",
    ").show(15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+----------+-------+------+------+------+----------+\n",
      "|Ticker|ParsedDate| Volume|  Open|   Low|  High|Close/Last|\n",
      "+------+----------+-------+------+------+------+----------+\n",
      "| BRK-B|2023-05-31|6175417|321.12|319.39|322.41|    321.08|\n",
      "| BRK-B|2023-05-30|3232461|321.86| 319.0|322.47|    322.19|\n",
      "| BRK-B|2023-05-26|3229873|320.44|319.67|322.63|     320.6|\n",
      "| BRK-B|2023-05-25|4251935|320.56|317.71|320.56|    319.02|\n",
      "| BRK-B|2023-05-24|3075393|322.71|319.56| 323.0|     320.2|\n",
      "| BRK-B|2023-05-23|4031342|328.19|322.97|329.27|    323.11|\n",
      "| BRK-B|2023-05-22|2763422|330.75|328.35|331.49|    329.13|\n",
      "| BRK-B|2023-05-19|4323538| 331.0|329.12|333.94|    330.39|\n",
      "| BRK-B|2023-05-18|2808329|326.87|325.85|329.98|    329.76|\n",
      "| BRK-B|2023-05-17|3047626|325.02|324.82|328.26|    327.39|\n",
      "| BRK-B|2023-05-16|2139996|322.46|322.36|324.69|    323.75|\n",
      "| BRK-B|2023-05-15|2191609|322.89|320.13|323.83|    323.53|\n",
      "| BRK-B|2023-05-12|1938264|323.82|320.54|324.24|    322.49|\n",
      "| BRK-B|2023-05-11|2549339| 321.0|319.81|322.96|    322.64|\n",
      "| BRK-B|2023-05-10|2641134|326.08|320.15|326.16|    322.99|\n",
      "| BRK-B|2023-05-09|2285924|324.87|323.48|326.88|    324.87|\n",
      "| BRK-B|2023-05-08|3303393|328.26|325.79|330.69|    326.14|\n",
      "| BRK-B|2023-05-05|3876299|323.36|322.62|325.16|    323.88|\n",
      "| BRK-B|2023-05-04|3194768|323.44|317.41|325.99|     320.0|\n",
      "| BRK-B|2023-05-03|2660456|327.13|323.06|328.07|    323.22|\n",
      "+------+----------+-------+------+------+------+----------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "cleaned_stocks.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+----------+-------+------+------+------+----------+----+-----+---+----+\n",
      "|Ticker|ParsedDate| Volume|  Open|   Low|  High|Close/Last|Year|Month|Day|Week|\n",
      "+------+----------+-------+------+------+------+----------+----+-----+---+----+\n",
      "| BRK-B|2023-05-31|6175417|321.12|319.39|322.41|    321.08|2023|    5| 31|  22|\n",
      "| BRK-B|2023-05-30|3232461|321.86| 319.0|322.47|    322.19|2023|    5| 30|  22|\n",
      "| BRK-B|2023-05-26|3229873|320.44|319.67|322.63|     320.6|2023|    5| 26|  21|\n",
      "| BRK-B|2023-05-25|4251935|320.56|317.71|320.56|    319.02|2023|    5| 25|  21|\n",
      "| BRK-B|2023-05-24|3075393|322.71|319.56| 323.0|     320.2|2023|    5| 24|  21|\n",
      "| BRK-B|2023-05-23|4031342|328.19|322.97|329.27|    323.11|2023|    5| 23|  21|\n",
      "| BRK-B|2023-05-22|2763422|330.75|328.35|331.49|    329.13|2023|    5| 22|  21|\n",
      "| BRK-B|2023-05-19|4323538| 331.0|329.12|333.94|    330.39|2023|    5| 19|  20|\n",
      "| BRK-B|2023-05-18|2808329|326.87|325.85|329.98|    329.76|2023|    5| 18|  20|\n",
      "| BRK-B|2023-05-17|3047626|325.02|324.82|328.26|    327.39|2023|    5| 17|  20|\n",
      "| BRK-B|2023-05-16|2139996|322.46|322.36|324.69|    323.75|2023|    5| 16|  20|\n",
      "| BRK-B|2023-05-15|2191609|322.89|320.13|323.83|    323.53|2023|    5| 15|  20|\n",
      "| BRK-B|2023-05-12|1938264|323.82|320.54|324.24|    322.49|2023|    5| 12|  19|\n",
      "| BRK-B|2023-05-11|2549339| 321.0|319.81|322.96|    322.64|2023|    5| 11|  19|\n",
      "| BRK-B|2023-05-10|2641134|326.08|320.15|326.16|    322.99|2023|    5| 10|  19|\n",
      "| BRK-B|2023-05-09|2285924|324.87|323.48|326.88|    324.87|2023|    5|  9|  19|\n",
      "| BRK-B|2023-05-08|3303393|328.26|325.79|330.69|    326.14|2023|    5|  8|  19|\n",
      "| BRK-B|2023-05-05|3876299|323.36|322.62|325.16|    323.88|2023|    5|  5|  18|\n",
      "| BRK-B|2023-05-04|3194768|323.44|317.41|325.99|     320.0|2023|    5|  4|  18|\n",
      "| BRK-B|2023-05-03|2660456|327.13|323.06|328.07|    323.22|2023|    5|  3|  18|\n",
      "+------+----------+-------+------+------+------+----------+----+-----+---+----+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import udf, col, year, month, dayofmonth, weekofyear\n",
    "from pyspark.sql.types import FloatType, DateType\n",
    "cleaned_stocks = (cleaned_stocks\n",
    "                  .withColumn(\"Year\", year(col(\"ParsedDate\")))\n",
    "                  .withColumn(\"Month\", month(col(\"ParsedDate\")))\n",
    "                  .withColumn(\"Day\", dayofmonth(col(\"ParsedDate\")))\n",
    "                  .withColumn(\"Week\", weekofyear(col(\"ParsedDate\"))))\n",
    "\n",
    "cleaned_stocks.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+----+--------+---------+\n",
      "|Ticker|Year|YearHigh|YearlyLow|\n",
      "+------+----+--------+---------+\n",
      "| BRK-B|2023|   331.0|   294.68|\n",
      "|  MSFT|2019|  159.45|    99.55|\n",
      "|  MSFT|2021|  344.62|   212.17|\n",
      "| BRK-B|2018|   224.0|   185.43|\n",
      "|  MSFT|2020|  229.27|   137.01|\n",
      "| BRK-B|2021|  300.88|   228.21|\n",
      "|  MSFT|2018|  115.42|    95.14|\n",
      "| BRK-B|2020|  233.92|    165.3|\n",
      "|  MSFT|2023|  335.23|    223.0|\n",
      "| BRK-B|2019|  227.27|   194.78|\n",
      "|  MSFT|2022|  335.35|   217.55|\n",
      "| BRK-B|2022|  361.39|   260.58|\n",
      "|  META|2020|  300.16|   139.75|\n",
      "|  META|2021|  381.68|    247.9|\n",
      "|  TSLA|2019|    29.0|    12.07|\n",
      "|  META|2018|  215.72|    123.1|\n",
      "|  TSLA|2021|  411.47|   184.18|\n",
      "|  TSLA|2018|    25.0|    17.02|\n",
      "|  META|2022|  339.95|    90.08|\n",
      "|  META|2019|  208.67|   128.99|\n",
      "+------+----+--------+---------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import max as max_ , sum as sum_ , min as min_\n",
    "yearly = cleaned_stocks.groupBy([\"Ticker\" , \"Year\"]).agg(max_(\"Open\").alias(\"YearHigh\") , min_(\"Open\").alias(\"YearlyLow\"))\n",
    "yearly.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+----+--------+---------+\n",
      "|Ticker|Week|WeekHigh|WeeklyLow|\n",
      "+------+----+--------+---------+\n",
      "|  MSFT|  28|  282.35|   101.15|\n",
      "| BRK-B|  53|   230.5|   228.09|\n",
      "|  MSFT|   9|  302.89|   111.26|\n",
      "| BRK-B|   6|   323.8|   200.82|\n",
      "|  MSFT|  30|  289.43|    106.3|\n",
      "| BRK-B|  15|  352.98|   184.61|\n",
      "|  MSFT|  20|  316.74|   123.87|\n",
      "| BRK-B|  40|  282.59|   202.65|\n",
      "|  MSFT|  19|  310.55|   124.29|\n",
      "| BRK-B|  46|  313.67|   214.83|\n",
      "|  MSFT|  18|  307.76|   127.36|\n",
      "| BRK-B|  43|  292.77|   199.01|\n",
      "|  MSFT|   7|  300.01|   106.14|\n",
      "| BRK-B|  16|  350.86|   188.56|\n",
      "|  MSFT|   2|  320.47|   101.64|\n",
      "|  MSFT|  40|  296.22|   112.63|\n",
      "|  MSFT|  41|  302.34|   105.35|\n",
      "| BRK-B|  22|  321.86|   179.99|\n",
      "|  MSFT|  49|  335.31|   105.82|\n",
      "| BRK-B|  27|  279.79|    175.9|\n",
      "+------+----+--------+---------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import max as max_ , sum as sum_ , min as min_\n",
    "weekly = cleaned_stocks.groupBy([\"Ticker\" , \"Week\"]).agg(max_(\"Open\").alias(\"WeekHigh\") , min_(\"Open\").alias(\"WeeklyLow\"))\n",
    "weekly.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+-----+---------+----------+\n",
      "|Ticker|Month|MonthHigh|MonthlyLow|\n",
      "+------+-----+---------+----------+\n",
      "|  MSFT|    9|   304.17|    108.23|\n",
      "| BRK-B|    6|    316.0|     175.9|\n",
      "|  MSFT|    7|   289.43|      98.1|\n",
      "|  MSFT|    2|   310.41|    102.87|\n",
      "| BRK-B|    1|   322.22|    194.78|\n",
      "|  MSFT|    8|   305.02|     105.4|\n",
      "| BRK-B|    9|    286.7|    201.19|\n",
      "|  MSFT|    3|   313.91|    109.16|\n",
      "| BRK-B|    8|   305.22|    195.78|\n",
      "|  MSFT|    6|    275.2|     97.38|\n",
      "| BRK-B|   10|   297.98|    199.01|\n",
      "|  MSFT|   10|   324.33|    103.66|\n",
      "|  MSFT|    5|   335.23|     99.29|\n",
      "| BRK-B|    3|   361.39|     165.3|\n",
      "| BRK-B|    7|   297.42|    178.26|\n",
      "| BRK-B|    5|    331.0|     168.8|\n",
      "|  MSFT|   12|   343.15|     95.14|\n",
      "|  MSFT|   11|   344.62|     101.8|\n",
      "|  MSFT|    1|   335.35|     99.55|\n",
      "| BRK-B|    2|    323.8|    200.82|\n",
      "+------+-----+---------+----------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import max as max_ , sum as sum_ , min as min_\n",
    "monthly = cleaned_stocks.groupBy([\"Ticker\" , \"Month\"]).agg(max_(\"Open\").alias(\"MonthHigh\") , min_(\"Open\").alias(\"MonthlyLow\"))\n",
    "monthly.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+---+-------+--------+\n",
      "|Ticker|Day|DayHigh|DaylyLow|\n",
      "+------+---+-------+--------+\n",
      "|  MSFT| 28| 343.15|   97.38|\n",
      "|  MSFT|  9| 337.11|  101.65|\n",
      "| BRK-B|  6| 341.17|  176.94|\n",
      "|  MSFT| 30| 341.91|  103.66|\n",
      "| BRK-B| 15|  332.7|  169.82|\n",
      "|  MSFT| 20| 320.05|  101.37|\n",
      "|  MSFT| 19| 342.64|   99.65|\n",
      "|  MSFT| 18| 338.18|  100.01|\n",
      "|  MSFT|  7| 331.64|  101.64|\n",
      "| BRK-B| 16| 335.11|   174.0|\n",
      "|  MSFT|  2| 330.31|    98.1|\n",
      "| BRK-B| 22| 351.01|  174.86|\n",
      "| BRK-B| 27|  330.6|   178.5|\n",
      "| BRK-B|  1| 353.65|  176.18|\n",
      "|  MSFT|  8|  337.3|  101.09|\n",
      "| BRK-B|  9| 326.59|  181.79|\n",
      "| BRK-B| 31|  359.0|  182.05|\n",
      "| BRK-B| 18| 344.45|   174.0|\n",
      "|  MSFT|  3| 335.35|   100.1|\n",
      "|  MSFT| 27| 335.46|    99.3|\n",
      "+------+---+-------+--------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import max as max_ , sum as sum_ , min as min_\n",
    "dayly = cleaned_stocks.groupBy([\"Ticker\" , \"Day\"]).agg(max_(\"Open\").alias(\"DayHigh\") , min_(\"Open\").alias(\"DaylyLow\"))\n",
    "dayly.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'historic_stock' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[71], line 9\u001b[0m\n\u001b[1;32m      7\u001b[0m monthly_alias \u001b[38;5;241m=\u001b[39m monthly\u001b[38;5;241m.\u001b[39malias(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mm\u001b[39m\u001b[38;5;124m\"\u001b[39m) \n\u001b[1;32m      8\u001b[0m cleaned_stocks_alias \u001b[38;5;241m=\u001b[39m cleaned_stocks\u001b[38;5;241m.\u001b[39malias(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcs\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m----> 9\u001b[0m historic_stock_alias \u001b[38;5;241m=\u001b[39m\u001b[43mhistoric_stock\u001b[49m\u001b[38;5;241m.\u001b[39malias(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mh\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     10\u001b[0m historic_stock_1_alias \u001b[38;5;241m=\u001b[39mhistoric_stock_1\u001b[38;5;241m.\u001b[39malias(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mh1\u001b[39m\u001b[38;5;124m\"\u001b[39m) \n\u001b[1;32m     11\u001b[0m historic_stock_2_alias \u001b[38;5;241m=\u001b[39mhistoric_stock_2\u001b[38;5;241m.\u001b[39malias(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mh2\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'historic_stock' is not defined"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import col\n",
    "\n",
    "# Gán alias cho các DataFrame\n",
    "weekly_alias = weekly.alias(\"w\")\n",
    "dayly_alias = dayly.alias(\"d\")\n",
    "yearly_alias = yearly.alias(\"y\")  # Sử dụng alias cho DataFrame yearly, không phải cho hàm hay biến year\n",
    "monthly_alias = monthly.alias(\"m\") \n",
    "cleaned_stocks_alias = cleaned_stocks.alias(\"cs\")\n",
    "historic_stock_alias =historic_stock.alias(\"h\")\n",
    "historic_stock_1_alias =historic_stock_1.alias(\"h1\") \n",
    "historic_stock_2_alias =historic_stock_2.alias(\"h2\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Analysis with week data\n",
    "from pyspark.sql.functions import col\n",
    "historic_stock = cleaned_stocks_alias.join(\n",
    "    weekly_alias,\n",
    "    (col(\"cs.Ticker\") == col(\"w.Ticker\")) & (col(\"cs.Week\") == col(\"w.Week\"))\n",
    "    , 'inner'\n",
    ").drop(col(\"w.Ticker\")).drop(col(\"w.Week\"))\n",
    "historic_stock.show(1)\n",
    "#cleaned join yearly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Analysis Stock with year data\n",
    "from pyspark.sql.functions import col\n",
    "historic_stock_1 = historic_stock_alias.join(\n",
    "   yearly_alias,\n",
    "    (col(\"h.Ticker\") == col(\"y.Ticker\")) & (col(\"h.Year\") == col(\"y.Year\"))\n",
    "    , 'inner'\n",
    ").drop(col(\"y.Ticker\")).drop(col(\"y.Year\"))\n",
    "historic_stock_1.show(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Join with monthly data\n",
    "from pyspark.sql.functions import col\n",
    "historic_stock_2 = historic_stock_1_alias.join(\n",
    "   monthly_alias,\n",
    "    (col(\"h1.Ticker\") == col(\"m.Ticker\")) & (col(\"h1.Month\") == col(\"m.Month\"))\n",
    "    , 'inner'\n",
    ").drop(col(\"m.Ticker\")).drop(col(\"m.Month\"))\n",
    "historic_stock_2.show(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# join with dayly data\n",
    "from pyspark.sql.functions import col\n",
    "historic_stock_3 = historic_stock_2_alias.join(\n",
    "   dayly_alias,\n",
    "    (col(\"h2.Ticker\") == col(\"d.Ticker\")) & (col(\"h2.Day\") == col(\"d.Day\"))\n",
    "    , 'inner'\n",
    ").drop(col(\"d.Ticker\")).drop(col(\"d.Day\"))\n",
    "historic_stock_3.show(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "historic_stock_3.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_stock = historic_stock_3.select(['Ticker', 'Year', 'Month', 'Day','Week', 'Volume' , 'Open' , 'Low', 'High', 'Close/Last' , 'YearHigh' , 'YearlyLow' , 'WeekHigh' ,'WeeklyLow' , 'MonthHigh' , 'MonthlyLow'])\n",
    "final_stock.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assuming 'final_stock' is your DataFrame containing stock databases\n",
    "final_stock.createOrReplaceTempView(\"StockDatabases\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assuming StockDatabases is your DataFrame\n",
    "query_1 = spark.sql(\"SELECT * FROM StockDatabases WHERE Ticker = 'BRK-B' AND Year = '2023'\")\n",
    "query_1.show(10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+----------+------+\n",
      "|Ticker|ParsedDate|  Open|\n",
      "+------+----------+------+\n",
      "| BRK-B|2023-05-31|321.12|\n",
      "| BRK-B|2023-05-30|321.86|\n",
      "| BRK-B|2023-05-26|320.44|\n",
      "| BRK-B|2023-05-25|320.56|\n",
      "| BRK-B|2023-05-24|322.71|\n",
      "| BRK-B|2023-05-23|328.19|\n",
      "| BRK-B|2023-05-22|330.75|\n",
      "| BRK-B|2023-05-19| 331.0|\n",
      "| BRK-B|2023-05-18|326.87|\n",
      "| BRK-B|2023-05-17|325.02|\n",
      "| BRK-B|2023-05-16|322.46|\n",
      "| BRK-B|2023-05-15|322.89|\n",
      "| BRK-B|2023-05-12|323.82|\n",
      "| BRK-B|2023-05-11| 321.0|\n",
      "| BRK-B|2023-05-10|326.08|\n",
      "| BRK-B|2023-05-09|324.87|\n",
      "| BRK-B|2023-05-08|328.26|\n",
      "| BRK-B|2023-05-05|323.36|\n",
      "| BRK-B|2023-05-04|323.44|\n",
      "| BRK-B|2023-05-03|327.13|\n",
      "+------+----------+------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "snapshot = cleaned_stocks.select(['Ticker' , 'ParsedDate' , 'Open'])\n",
    "snapshot.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.window import Window\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "windowSpec = Window.partitionBy(\"Ticker\").orderBy(\"ParsedDate\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+----------+-----+------------+\n",
      "|Ticker|ParsedDate| Open|PreviousOpen|\n",
      "+------+----------+-----+------------+\n",
      "|  AAPL|2018-05-31|46.81|        null|\n",
      "|  AAPL|2018-06-01| 47.0|       46.81|\n",
      "|  AAPL|2018-06-04|47.91|        47.0|\n",
      "|  AAPL|2018-06-05|48.27|       47.91|\n",
      "|  AAPL|2018-06-06|48.41|       48.27|\n",
      "|  AAPL|2018-06-07|48.54|       48.41|\n",
      "|  AAPL|2018-06-08|47.79|       48.54|\n",
      "|  AAPL|2018-06-11|47.84|       47.79|\n",
      "|  AAPL|2018-06-12|47.85|       47.84|\n",
      "|  AAPL|2018-06-13|48.11|       47.85|\n",
      "|  AAPL|2018-06-14|47.89|       48.11|\n",
      "|  AAPL|2018-06-15|47.51|       47.89|\n",
      "|  AAPL|2018-06-18|46.97|       47.51|\n",
      "|  AAPL|2018-06-19|46.29|       46.97|\n",
      "|  AAPL|2018-06-20|46.59|       46.29|\n",
      "|  AAPL|2018-06-21|46.81|       46.59|\n",
      "|  AAPL|2018-06-22|46.53|       46.81|\n",
      "|  AAPL|2018-06-25|45.85|       46.53|\n",
      "|  AAPL|2018-06-26|45.75|       45.85|\n",
      "|  AAPL|2018-06-27|46.31|       45.75|\n",
      "+------+----------+-----+------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import lag as lag_\n",
    "snapshot.withColumn(\"PreviousOpen\" , lag_(\"Open\").over(windowSpec)).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Caculate moving average\n",
    "movingAvagare = Window.partitionBy(\"Ticker\").orderBy(\"ParsedDate\").rowsBetween(-50 , 0 ) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+----------+-----+-----+\n",
      "|Ticker|ParsedDate| Open| MA50|\n",
      "+------+----------+-----+-----+\n",
      "|  AAPL|2018-05-31|46.81|46.81|\n",
      "|  AAPL|2018-06-01| 47.0|46.91|\n",
      "|  AAPL|2018-06-04|47.91|47.24|\n",
      "+------+----------+-----+-----+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import avg as avg_ , round as round_ \n",
    "movingAVG =(snapshot.withColumn(\"MA50\" , avg_(\"Open\").over(movingAvagare)).withColumn(\"MA50\" , round_(\"MA50\" , 2)))\n",
    "movingAVG.show(3)\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+----------+------+-------+\n",
      "|Ticker|ParsedDate|  Open|MaxOpen|\n",
      "+------+----------+------+-------+\n",
      "|  AAPL|2022-01-04|182.63|      1|\n",
      "|  AAPL|2021-12-13|181.12|      2|\n",
      "|  AAPL|2021-12-28|180.16|      3|\n",
      "|  AAPL|2022-01-05|179.61|      4|\n",
      "|  AAPL|2021-12-30|179.47|      5|\n",
      "|  AAPL|2021-12-29|179.33|      6|\n",
      "|  AAPL|2021-12-16|179.28|      7|\n",
      "|  AAPL|2022-03-30|178.55|      8|\n",
      "|  AAPL|2021-12-31|178.09|      9|\n",
      "|  AAPL|2022-03-31|177.84|     10|\n",
      "|  AAPL|2022-01-03|177.83|     11|\n",
      "|  AAPL|2022-04-05| 177.5|     12|\n",
      "|  AAPL|2023-05-31|177.33|     13|\n",
      "|  AAPL|2021-12-27|177.09|     14|\n",
      "|  AAPL|2023-05-30|176.96|     15|\n",
      "|  AAPL|2022-03-29|176.69|     16|\n",
      "|  AAPL|2023-05-19|176.39|     17|\n",
      "|  AAPL|2022-01-12|176.12|     18|\n",
      "|  AAPL|2022-02-09|176.05|     19|\n",
      "|  AAPL|2021-12-23|175.85|     20|\n",
      "+------+----------+------+-------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import desc , row_number as rn\n",
    "maximumStock = Window.partitionBy(\"Ticker\").orderBy(snapshot.Open.desc())\n",
    "result =  snapshot.withColumn(\"MaxOpen\",  rn().over(maximumStock))\n",
    "result.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Saving data \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "#Parquet format\n",
    "(result.write.options(header=True)\n",
    "        .partitionBy(\"Ticker\", \"ParsedDate\")\n",
    "        .mode(\"overwrite\")\n",
    "        .csv(\"result_csv\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "(result.write.options(header=True)\n",
    "        .partitionBy(\"Ticker\", \"ParsedDate\")\n",
    "        .mode(\"overwrite\")\n",
    "        .parquet(\"parquet_csv\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "(movingAVG.write.options(header=True)\n",
    "        .partitionBy(\"Ticker\", \"ParsedDate\")\n",
    "        .mode(\"overwrite\")\n",
    "        .csv(\"movingAVG_csv\"))"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"
  },
  "kernelspec": {
   "display_name": "Python 3.11.4 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
